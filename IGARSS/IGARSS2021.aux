\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Zhu2017_Deep}
\citation{yang2018geodesic}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1} Introduction}{1}{section.1}}
\newlabel{sec:Introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2} Variational Autoencoders }{1}{section.2}}
\citation{Murphy2019_Unsupervised}
\citation{Maggioni2019_LUND}
\citation{Murphy2020_Spectral}
\citation{Maggioni2019_LAND}
\citation{Maggioni2019_LAND}
\citation{Murphy2019_Unsupervised}
\citation{Maggioni2019_LAND}
\citation{Murphy2012_Machine}
\citation{ehsan2017infinite}
\citation{makhzani2015adversarial}
\citation{tian2014learning}
\citation{song2013auto}
\citation{xie2016unsupervised}
\citation{jiang2017variational}
\citation{chen2014deep}
\citation{chen2016deep}
\citation{li2017spectral}
\citation{he2017multi}
\citation{paoletti2019deep}
\citation{zhang20191d}
\citation{cohn1995active}
\citation{mackay1992information}
\citation{liu2016active}
\citation{wang2017novel}
\citation{murphy2018iterative}
\citation{tuia2009active}
\citation{tuia2011survey}
\citation{pourkamali2019effectiveness}
\citation{pourkamali2019effectiveness}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1} Learning by Active Nonlinear Diffusion}{2}{subsection.2.1}}
\newlabel{eqn:rho}{{1}{2}{Learning by Active Nonlinear Diffusion}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2} Related Work}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3} Proposed Algorithm}{3}{section.3}}
\newlabel{sec:ProposedAlgorithm}{{3}{3}{Proposed Algorithm}{section.3}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:VALAND}{{1}{3}{Proposed Algorithm}{algocf.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Variational Autoencoder Learning by Active Nonlinear Diffusion (VALAND)\relax }}{3}{algocf.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4} Experimental Results}{3}{section.4}}
\newlabel{sec:Experiments}{{4}{3}{Experimental Results}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {The $86\times 83$ Salinas A HSI data consists of 6 classes. Left: the sum of all spectral bands. Right: the ground truth.}\relax }}{3}{figure.caption.2}}
\newlabel{fig:SalinasA}{{1}{3}{\small {The $86\times 83$ Salinas A HSI data consists of 6 classes. Left: the sum of all spectral bands. Right: the ground truth.}\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {A schematics of the VAE archiecture. Input is cascaded through an encoder, three dense layers of size 128. The extraccted latent feature is fed to the decoder,three dense layers of size 128, to obtain the output. Note, unlike the standard autoencoder, the extraction of the latent feature is not deterministic. }\relax }}{3}{figure.caption.3}}
\newlabel{fig:SalinasA}{{2}{3}{\small {A schematics of the VAE archiecture. Input is cascaded through an encoder, three dense layers of size 128. The extraccted latent feature is fed to the decoder,three dense layers of size 128, to obtain the output. Note, unlike the standard autoencoder, the extraction of the latent feature is not deterministic. }\relax }{figure.caption.3}{}}
\citation{Maggioni2019_LAND}
\bibstyle{IEEEbib}
\bibdata{IGARSS2021_ref}
\bibcite{Zhu2017_Deep}{1}
\bibcite{yang2018geodesic}{2}
\bibcite{Murphy2019_Unsupervised}{3}
\bibcite{Maggioni2019_LUND}{4}
\bibcite{Murphy2020_Spectral}{5}
\bibcite{Maggioni2019_LAND}{6}
\bibcite{Murphy2012_Machine}{7}
\bibcite{ehsan2017infinite}{8}
\bibcite{makhzani2015adversarial}{9}
\bibcite{tian2014learning}{10}
\bibcite{song2013auto}{11}
\bibcite{xie2016unsupervised}{12}
\bibcite{jiang2017variational}{13}
\bibcite{chen2014deep}{14}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces For the Salinas A dataset, the performance of variational autoencoder LAND learning achieves a higher accuracy than the standard LAND algorithm. With just $10$ points, the overall accuracy of VAE-LAND is 96.97\%, a 12.5\% improvement to the competitive LAND algorithm. Both VAE-LAND and LAND obtain significantly better results than using randomly selected training instances. \relax }}{4}{figure.caption.4}}
\newlabel{fig:my_label}{{3}{4}{For the Salinas A dataset, the performance of variational autoencoder LAND learning achieves a higher accuracy than the standard LAND algorithm. With just $10$ points, the overall accuracy of VAE-LAND is 96.97\%, a 12.5\% improvement to the competitive LAND algorithm. Both VAE-LAND and LAND obtain significantly better results than using randomly selected training instances. \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5} Conclusions and Future Directions}{4}{section.5}}
\newlabel{sec:Conclusions}{{5}{4}{Conclusions and Future Directions}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6} References}{4}{section.6}}
\bibcite{chen2016deep}{15}
\bibcite{li2017spectral}{16}
\bibcite{he2017multi}{17}
\bibcite{paoletti2019deep}{18}
\bibcite{zhang20191d}{19}
\bibcite{cohn1995active}{20}
\bibcite{mackay1992information}{21}
\bibcite{liu2016active}{22}
\bibcite{wang2017novel}{23}
\bibcite{murphy2018iterative}{24}
\bibcite{tuia2009active}{25}
\bibcite{tuia2011survey}{26}
\bibcite{pourkamali2019effectiveness}{27}
