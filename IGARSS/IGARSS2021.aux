\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Zhu2017_Deep}
\citation{kingma2014auto}
\citation{Murphy2019_Unsupervised}
\citation{Maggioni2019_LUND}
\citation{Murphy2020_Spectral}
\citation{Maggioni2019_LAND}
\citation{Maggioni2019_LAND}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1} Introduction}{1}{section.1}}
\newlabel{sec:Introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2} Background}{1}{section.2}}
\newlabel{sec:vaes}{{2}{1}{Background}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1} Variational Autoencoders}{1}{subsection.2.1}}
\newlabel{subsec:vaes}{{2.1}{1}{Variational Autoencoders}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2} Learning by Active Nonlinear Diffusion}{1}{subsection.2.2}}
\citation{Maggioni2019_LAND}
\citation{ehsan2017infinite}
\citation{makhzani2015adversarial}
\citation{tian2014learning}
\citation{song2013auto}
\citation{xie2016unsupervised}
\citation{chen2014deep}
\citation{chen2016deep}
\citation{li2017spectral}
\citation{he2017multi}
\citation{paoletti2019deep}
\citation{cohn1995active}
\citation{mackay1992information}
\citation{liu2016active}
\citation{wang2017novel}
\citation{murphy2018iterative}
\citation{tuia2009active}
\citation{pourkamali2019effectiveness}
\citation{li2020variational}
\newlabel{eqn:rho}{{1}{2}{Learning by Active Nonlinear Diffusion}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3} Related Work}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3} Proposed Algorithm}{2}{section.3}}
\newlabel{sec:ProposedAlgorithm}{{3}{2}{Proposed Algorithm}{section.3}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:VALAND}{{1}{2}{Proposed Algorithm}{algocf.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Variational Autoencoder Learning by Active Nonlinear Diffusion (VAE-LAND)\relax }}{2}{algocf.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4} Experimental Results}{2}{section.4}}
\newlabel{sec:Experiments}{{4}{2}{Experimental Results}{section.4}{}}
\citation{Maggioni2019_LAND}
\bibstyle{IEEEbib}
\bibdata{IGARSS2021_ref}
\bibcite{Zhu2017_Deep}{1}
\bibcite{kingma2014auto}{2}
\bibcite{Murphy2019_Unsupervised}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {The $86\times 83$ Salinas A HSI data consists of 6 classes. \emph  {Left:} the sum of all spectral bands. \emph  {Right:} the ground truth.}\relax }}{3}{figure.caption.2}}
\newlabel{fig:SalinasA}{{1}{3}{\small {The $86\times 83$ Salinas A HSI data consists of 6 classes. \emph {Left:} the sum of all spectral bands. \emph {Right:} the ground truth.}\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {A schematic of the VAE architecture. Input is a vector in ${\mathbb  R}^{224}$. The encoder and decoder are fully connected neural networks. Both have three layers with 128 neurons in each layer. For all layers, the activation function is the rectified linear unit (ReLU). The input is cascaded through an encoder. The extracted latent feature in ${\mathbb  R}^{40}$ is then cascaded through the decoder to obtain the output vector in ${\mathbb  R}^{224}$. Unlike the standard autoencoder, the extraction of the latent feature is not deterministic (see Section \ref  {subsec:vaes} for discussion.)}\relax }}{3}{figure.caption.3}}
\newlabel{fig:vae_schematics}{{2}{3}{\small {A schematic of the VAE architecture. Input is a vector in $\R ^{224}$. The encoder and decoder are fully connected neural networks. Both have three layers with 128 neurons in each layer. For all layers, the activation function is the rectified linear unit (ReLU). The input is cascaded through an encoder. The extracted latent feature in $\R ^{40}$ is then cascaded through the decoder to obtain the output vector in $\R ^{224}$. Unlike the standard autoencoder, the extraction of the latent feature is not deterministic (see Section \ref {subsec:vaes} for discussion.)}\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5} Conclusions and Future Directions}{3}{section.5}}
\newlabel{sec:Conclusions}{{5}{3}{Conclusions and Future Directions}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6} References}{3}{section.6}}
\bibcite{Maggioni2019_LUND}{4}
\bibcite{Murphy2020_Spectral}{5}
\bibcite{Maggioni2019_LAND}{6}
\bibcite{ehsan2017infinite}{7}
\bibcite{makhzani2015adversarial}{8}
\bibcite{tian2014learning}{9}
\bibcite{song2013auto}{10}
\bibcite{xie2016unsupervised}{11}
\bibcite{chen2014deep}{12}
\bibcite{chen2016deep}{13}
\bibcite{li2017spectral}{14}
\bibcite{he2017multi}{15}
\bibcite{paoletti2019deep}{16}
\bibcite{cohn1995active}{17}
\bibcite{mackay1992information}{18}
\bibcite{liu2016active}{19}
\bibcite{wang2017novel}{20}
\bibcite{murphy2018iterative}{21}
\bibcite{tuia2009active}{22}
\bibcite{pourkamali2019effectiveness}{23}
\bibcite{li2020variational}{24}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces For the Salinas A dataset, the performance of VAE-LAND learning achieves a higher accuracy than the standard LAND algorithm. With just $10$ points, the overall accuracy of VAE-LAND is 96.97\%, a 12.5\% improvement to the competitive LAND algorithm. Both VAE-LAND and LAND obtain significantly better results than using randomly selected training instances. \relax }}{4}{figure.caption.4}}
\newlabel{fig:my_label}{{3}{4}{For the Salinas A dataset, the performance of VAE-LAND learning achieves a higher accuracy than the standard LAND algorithm. With just $10$ points, the overall accuracy of VAE-LAND is 96.97\%, a 12.5\% improvement to the competitive LAND algorithm. Both VAE-LAND and LAND obtain significantly better results than using randomly selected training instances. \relax }{figure.caption.4}{}}
